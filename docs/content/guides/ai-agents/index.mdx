---
title: Building Durable AI Agents
---

AI agents are built on the primitive of LLM and tool-call loops, often with additional processes for data fetching, resource provisioning, or reacting to external events.

Workflow DevKit makes your agents production-ready, by turning them into durable, resumable workflows, and managing your LLM calls, tool executions, and other async operations as retryable and observable steps.

<AgentTraces />

This guide walks you through converting a basic AI chat app into a durable AI agent using Workflow DevKit.

## Why Durable Agents?

Aside from the usual challenges of getting your long-running tasks to be production-ready, building mature AI agents typically requires solving several **additional challenges**:

- **Statefulness**: Persisting chat sessions and turning LLM and tool calls into async jobs with workers and queues.
- **Observability**: Using services to collect traces and metrics, and managing them separately from your messages and user history.
- **Resumability**: Resuming streams requires not just storing your messages, but also storing streams, and piping them across services.
- **Human-in-the-loop**: Your client, API, and async job orchestration need to work together to create, track, route to, and display human approval requests, or similar webhook operations.

Workflow DevKit provides all of these capabilities out of the box. Your agent becomes a workflow, your tools become steps, and the framework handles interplay with your existing infrastructure.

## Getting Started

To make an Agent durable, we first need an Agent, which we'll be setting up here. If you already have an app you'd like to follow along with, you can skip this section.

For our example, we'll need an app with a simple chat interface and an API route calling an LLM, so that we can add Workflow DevKit to it. We'll use the [Flight Booking Agent](https://github.com/vercel/workflow-examples/tree/main/flight-booking-app) example as a starting point, which comes with a chat interface built using Next.js, AI SDK, and Shadcn UI.

<Steps>

<Step>
### Clone example app

We'll need an app with a simple chat interface and an API route calling an LLM, so that we can add Workflow DevKit to it. For the follow-along steps, we'll use the [Flight Booking Agent](https://github.com/vercel/workflow-examples/tree/main/flight-booking-app) example as a starting point, which comes with a chat interface built using Next.js, AI SDK, and Shadcn UI.

If you have your own project, you can skip this step, and simply apply the changes of the following steps to your own project.

```bash
git clone https://github.com/vercel/workflow-examples -b plain-ai-sdk
cd workflow-examples/flight-booking-app
```

</Step>

<Step>

### Set up API keys

In order to connect to an LLM, we'll need to set up an API key. The easiest way to do this is to use a Vercel Gateway (works with all providers at zero markup), or you can configure a custom provider.
<Tabs items={['Gateway', 'Custom Provider']}>

<Tab value="Gateway">

Get a Gateway API key from the [Vercel Gateway](https://vercel.com/docs/gateway/api-reference/overview) page.

Then add it to your `.env.local` file:

```bash title=".env.local" lineNumbers
GATEWAY_API_KEY=...
```

</Tab>

<Tab value="Custom Provider">

This is an example of how to use the OpenAI provider for AI SDK. For details on other providers and more details, see the [AI SDK provider guide](https://ai-sdk.dev/providers/ai-sdk-providers).

```package-install
npm i @ai-sdk/openai
```

Set your OpenAI API key in your environment variables:

```bash title=".env.local" lineNumbers
OPENAI_API_KEY=...
```

Then modify your API endpoint to use the OpenAI provider:

```typescript title="app/api/chat/route.ts" lineNumbers
// ...
import { openai } from '@ai-sdk/openai'; // [!code highlight]

// This uses the OPENAI_API_KEY environment variable by default, but you
// can also pass { apiKey: string } to the createOpenAI function.
const openai = createOpenAI();

export async function POST(req: Request) {
  // ...
  const agent = new Agent({
    model: openai('gpt-5.1'), // [!code highlight]
    // ...
  });
```

</Tab>
</Tabs>
</Step>

<Step>

### Get familiar with the code

Let's take a moment to see what we're working with. Run the app with `npm run dev` and open [http://localhost:3000](http://localhost:3000) in your browser. You should see a simple chat interface to play with. Go ahead and give it a try.

The core code that makes all of this happen is quite simple. Here's a breakdown of the main parts:

<Tabs items={['API Route', 'Tools', 'Client']}>

<Tab value="API Route">

Our API route makes a simple call to [AI SDK's `Agent` class](https://ai-sdk.dev/docs/agents/overview), which is a simple wrapper around [AI SDK's `streamText` function](https://ai-sdk.dev/docs/reference/ai-sdk-core/stream-text#streamtext). This is also where we pass tools to the agent.

```typescript title="app/api/chat/route.ts" lineNumbers
export async function POST(req: Request) {
  const { messages }: { messages: UIMessage[] } = await req.json();
  const agent = new Agent({ // [!code highlight]
    model: gateway('bedrock/claude-4-5-haiku-20251001-v1'),
    system: FLIGHT_ASSISTANT_PROMPT,
    tools: flightBookingTools,
  });
  const modelMessages = convertToModelMessages(messages);
  const stream = agent.stream({ messages: modelMessages }); // [!code highlight]
  return createUIMessageStreamResponse({
    stream: stream.toUIMessageStream(),
  });
}
```

</Tab>

<Tab value="Tools">

Our tools are mostly mocked out for the sake of the example. We use AI SDK's `tool` function to define the tool, and pass it to the agent. In your own app, this might be any kind of tool call, like database queries, calls to external services, etc.

```typescript title="workflows/chat/steps/tools.ts" lineNumbers
import { tool } from 'ai';
import { z } from 'zod';

export const tools = {
  searchFlights: tool({
    description: 'Search for flights',
    inputSchema: z.object({ query: z.string() }),
    execute: searchFlights,
  }),
};

async function searchFlights({ from, to, date }: { from: string; to: string; date: string }) {
  // ... generate some fake flights
}
```

</Tab>

<Tab value="Client">

Our `ChatPage` component has a lot of logic for nicely displaying the chat messages, but at it's core, it's simply managing input/output for the [`useChat` hook](https://ai-sdk.dev/docs/reference/ai-sdk-ui/use-chat#usechat) from AI SDK.

```typescript title="app/chat.tsx" lineNumbers
'use client';

import { useChat } from '@ai-sdk/react';

export default function ChatPage() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({ // [!code highlight]
    api: '/api/chat',
  });

  // ... more UI logic

  return (
    <div>
      // This is a simplified example of the rendering logic
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role}:</strong>
          {m.parts.map((part, i) => {
            if (part.type === 'text') { // [!code highlight]
              return <span key={i}>{part.text}</span>;
            }
            if (part.type === 'tool-searchFlights') { // [!code highlight]
              // ... some special rendering for our tool results
            }
            return null;
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
      </form>
    </div>
  );
}
```

</Tab>

</Tabs>

</Step>

</Steps>

## Integrating Workflow DevKit

<Steps>
<Step>

### Install Dependencies

Add the Workflow DevKit packages to your project:

```package-install
npm i workflow @workflow/ai
```

and extend the Next.js config to transform your workflow code (see [Getting Started](/docs/getting-started/next) for more details).

```typescript title="next.config.ts" lineNumbers
import { withWorkflow } from 'workflow/next';
import type { NextConfig } from 'next';

const nextConfig: NextConfig = {
  // ... rest of your Next.js config
};

export default withWorkflow(nextConfig);
```

</Step>

<Step>
### Create a Workflow Function

Move the agent logic into a separate workflow function:

```typescript title="app/api/chat/workflow.ts" lineNumbers
import { DurableAgent } from '@workflow/ai/agent'; // [!code highlight]
import { getWritable } from 'workflow'; // [!code highlight]
import { stepCountIs } from 'ai';
import { tools } from '@/ai/tools';
import type { ModelMessage, UIMessageChunk } from 'ai';

export async function chatWorkflow({
  messages,
  modelId,
}: {
  messages: ModelMessage[];
  modelId: string;
}) {
  'use workflow'; // [!code highlight]

  const writable = getWritable<UIMessageChunk>(); // [!code highlight]

  const agent = new DurableAgent({ // [!code highlight]
    model: modelId,
    system: 'You are a helpful assistant.',
    tools: tools(),
  });

  await agent.stream({ // [!code highlight]
    messages,
    writable,
    stopWhen: stepCountIs(20),
  });
}
```

Key changes:

- Replace `Agent` with [`DurableAgent`](/docs/api-reference/workflow-ai/durable-agent) from `@workflow/ai/agent`
- Add the `"use workflow"` directive to mark this as a workflow function
- Use [`getWritable()`](/docs/api-reference/workflow/get-writable) to get a stream for agent output
- Pass the `writable` to `agent.stream()` instead of returning a stream directly
</Step>

<Step>
### Update the API Route

Replace the agent call with [`start()`](/docs/api-reference/workflow-api/start) to run the workflow:

```typescript title="app/api/chat/route.ts" lineNumbers
import { createUIMessageStreamResponse, convertToModelMessages } from 'ai';
import { start } from 'workflow/api'; // [!code highlight]
import { chatWorkflow } from './workflow'; // [!code highlight]

export async function POST(req: Request) {
  const { messages, modelId } = await req.json();
  const modelMessages = convertToModelMessages(messages);

  const run = await start(chatWorkflow, [{ messages: modelMessages, modelId }]); // [!code highlight]

  return createUIMessageStreamResponse({
    stream: run.readable, // [!code highlight]
  });
}
```

</Step>

<Step>
### Convert Tools to Steps

Mark tool execution functions with `"use step"` to make them durable. This enables automatic retries and observability:

```typescript title="ai/tools/search-web.ts" lineNumbers
import { tool } from 'ai';
import { z } from 'zod';

async function executeSearch({ query }: { query: string }) {
  'use step'; // [!code highlight]

  const response = await fetch(`https://api.search.com?q=${query}`);
  return response.json();
}

export const searchWeb = tool({
  description: 'Search the web for information',
  inputSchema: z.object({ query: z.string() }),
  execute: executeSearch,
});
```

With `"use step"`:

- The tool execution runs in a separate step with full Node.js access
- Failed tool calls are automatically retried (up to 3 times by default)
- Each tool execution appears as a discrete step in observability tools
- Results are persisted, so replays skip already-completed tools
</Step>

<Step>
### Stream Progress Updates from Tools

Tools can emit progress updates to the same stream the agent uses. This allows the UI to display tool status.

<Tabs items={['Tool', 'Client']}>

<Tab value="Tool">

```typescript title="ai/tools/search-web.ts" lineNumbers
import { tool } from 'ai';
import { getWritable } from 'workflow';
import { z } from 'zod';
import type { UIMessageChunk } from 'ai';

async function executeSearch(
  { url }: { url: string },
  { toolCallId }: { toolCallId: string }
) {
  'use step';

  const writable = getWritable<UIMessageChunk>(); // [!code highlight]
  const writer = writable.getWriter(); // [!code highlight]

  // Emit a progress update // [!code highlight]
  await writer.write({ // [!code highlight]
    id: toolCallId, // [!code highlight]
    type: 'data-search-web', // [!code highlight]
    data: { url, status: 'fetching' }, // [!code highlight]
  }); // [!code highlight]

  const response = await fetch(url);
  const content = await response.text();

  await writer.write({ // [!code highlight]
    id: toolCallId, // [!code highlight]
    type: 'data-search-web', // [!code highlight]
    data: { url, status: 'done' }, // [!code highlight]
  }); // [!code highlight]

  writer.releaseLock(); // [!code highlight]

  return content;
}

export const searchWeb = tool({
  description: 'Read a web page and return the content',
  inputSchema: z.object({ url: z.string() }),
  execute: executeSearch,
});
```

</Tab>

<Tab value="Client">

Handle the `data-search-web` chunks in your client to display progress. Data parts are stored in the message, so you can find the latest status directly:

```typescript title="app/chat.tsx" lineNumbers
'use client';

import { useChat } from '@ai-sdk/react';

export function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat',
  });

  return (
    <div>
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role}:</strong>
          {m.parts.map((part, i) => {
            if (part.type === 'text') {
              return <span key={i}>{part.text}</span>;
            }
            if (part.type === 'tool-invocation') {
              // Find the latest data part for this tool call // [!code highlight]
              const dataPart = m.parts.findLast( // [!code highlight]
                (p) => p.type === 'data' && p.id === part.toolInvocation.toolCallId // [!code highlight]
              ); // [!code highlight]
              const status = dataPart?.type === 'data' ? dataPart.data : null; // [!code highlight]
              return (
                <div key={i}>
                  {status?.status === 'fetching' // [!code highlight]
                    ? `Fetching ${status.url}...` // [!code highlight]
                    : `Called ${part.toolInvocation.toolName}`} // [!code highlight]
                </div>
              );
            }
            return null;
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
      </form>
    </div>
  );
}
```

</Tab>

</Tabs>

</Step>

</Steps>

That's all you need to do to convert your basic AI SDK agent into a durable agent. If you run your development server, and send a chat message, you should see your agent respond just as before, but now with added durability and observability.

## Observability

In your app directory, you can open up the observability dashboard to see your workflow in action, using the CLI:

```bash
npx workflow web
```

This opens a local dashboard showing all workflow runs and their status, as well as a trace viewer to inspect the workflow in detail, including retry attempts, and the data being passed between steps.

If you'd like to have terminal interface instead, you can use the CLI directly:

```bash
npx workflow inspect runs
```

## Next Steps

Now that you have a basic durable agent, it's a only a short step to add these additional features:

<Cards>
  <Card title="Resumable Streams" href="/guides/ai-agents/resumable-streams">
    Enable clients to reconnect to interrupted streams without losing data.
  </Card>
  <Card title="Sleep and Delays" href="/guides/ai-agents/sleep-and-delays">
    Add native sleep and scheduling functionality to your Agent and workflow.
  </Card>
  <Card title="Human-in-the-Loop" href="/guides/ai-agents/human-in-the-loop">
    Implement approval steps to wait for human input or external events.
  </Card>
</Cards>

## Complete Example

Here is the complete code for the durable agent after all the steps above:

<Tabs items={['API Route', 'Workflow', 'Tools', 'Client']}>

<Tab value="API Route">

```typescript title="app/api/chat/route.ts" lineNumbers
import { createUIMessageStreamResponse, convertToModelMessages } from 'ai';
import { start } from 'workflow/api';
import { chatWorkflow } from './workflow';

export async function POST(req: Request) {
  const { messages, modelId } = await req.json();
  const modelMessages = convertToModelMessages(messages);

  const run = await start(chatWorkflow, [{ messages: modelMessages, modelId }]);

  return createUIMessageStreamResponse({
    stream: run.readable,
  });
}
```

</Tab>

<Tab value="Workflow">

```typescript title="app/api/chat/workflow.ts" lineNumbers
import { DurableAgent } from '@workflow/ai/agent';
import { getWritable } from 'workflow';
import { stepCountIs } from 'ai';
import { tools } from '@/ai/tools';
import type { ModelMessage, UIMessageChunk } from 'ai';

export async function chatWorkflow({
  messages,
  modelId,
}: {
  messages: ModelMessage[];
  modelId: string;
}) {
  'use workflow';

  const writable = getWritable<UIMessageChunk>();

  const agent = new DurableAgent({
    model: modelId,
    system: 'You are a helpful assistant.',
    tools,
  });

  await agent.stream({
    messages,
    writable,
    stopWhen: stepCountIs(20),
  });
}
```

</Tab>

<Tab value="Tools">

```typescript title="ai/tools/index.ts" lineNumbers
import { tool } from 'ai';
import { getWritable } from 'workflow';
import { z } from 'zod';
import type { UIMessageChunk } from 'ai';

async function executeSearch(
  { url }: { url: string },
  { toolCallId }: { toolCallId: string }
) {
  'use step';

  const writable = getWritable<UIMessageChunk>();
  const writer = writable.getWriter();

  await writer.write({
    id: toolCallId,
    type: 'data-search-web',
    data: { url, status: 'fetching' },
  });

  const response = await fetch(url);
  const content = await response.text();

  await writer.write({
    id: toolCallId,
    type: 'data-search-web',
    data: { url, status: 'done' },
  });

  writer.releaseLock();

  return content;
}

export const tools = {
  searchWeb: tool({
    description: 'Read a web page and return the content',
    inputSchema: z.object({ url: z.string() }),
    execute: executeSearch,
  }),
};
```

</Tab>

<Tab value="Client">

```typescript title="app/chat.tsx" lineNumbers
'use client';

import { useChat } from '@ai-sdk/react';

export function Chat() {
  const { messages, input, handleInputChange, handleSubmit } = useChat({
    api: '/api/chat',
  });

  return (
    <div>
      {messages.map((m) => (
        <div key={m.id}>
          <strong>{m.role}:</strong>
          {m.parts.map((part, i) => {
            if (part.type === 'text') {
              return <span key={i}>{part.text}</span>;
            }
            if (part.type === 'tool-invocation') {
              // Find the latest data part for this tool call
              const dataPart = m.parts.findLast(
                (p) => p.type === 'data' && p.id === part.toolInvocation.toolCallId
              );
              const status = dataPart?.type === 'data' ? dataPart.data : null;
              return (
                <div key={i}>
                  {status?.status === 'fetching'
                    ? `Fetching ${status.url}...`
                    : `Called ${part.toolInvocation.toolName}`}
                </div>
              );
            }
            return null;
          })}
        </div>
      ))}
      <form onSubmit={handleSubmit}>
        <input
          value={input}
          onChange={handleInputChange}
          placeholder="Type a message..."
        />
      </form>
    </div>
  );
}
```

</Tab>

</Tabs>

## Related Documentation

- [Defining Tools](/docs/ai-agents/defining-tools) - Tool patterns and execute context
- [`DurableAgent` API Reference](/docs/api-reference/workflow-ai/durable-agent) - Full API documentation
- [Workflows and Steps](/docs/foundations/workflows-and-steps) - Core concepts
- [Streaming](/docs/foundations/streaming) - In-depth streaming guide
- [Errors and Retries](/docs/foundations/errors-and-retries) - Error handling patterns
